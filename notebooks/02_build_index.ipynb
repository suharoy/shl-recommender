{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803f21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Suha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Suha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Suha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 assessments from ../data/catalog_clean.csv\n",
      "Creating sentence embeddings (MiniLM-L6-v2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and metadata saved.\n",
      "Building BM25 index...\n",
      "BM25 index saved.\n",
      "\n",
      "Example query test:\n",
      "                                    name test_type\n",
      "23                         ADO.NET (New)   Unknown\n",
      "22  Accounts Receivable Simulation (New)   Unknown\n",
      "21             Accounts Receivable (New)   Unknown\n",
      "20     Accounts Payable Simulation (New)   Unknown\n",
      "19                Accounts Payable (New)   Unknown\n",
      "Hybrid search test completed. Sample results saved.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook builds hybrid retrieval components for the SHL recommender:\n",
    "- Dense embeddings (SentenceTransformer + FAISS)\n",
    "- Sparse index (BM25)\n",
    "- Hybrid scoring function\n",
    "\n",
    "Output:\n",
    " - data/embeddings.faiss\n",
    " - data/meta_catalog.json\n",
    " - data/bm25.pkl (optional)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ----------------------------\n",
    "# Load catalog\n",
    "# ----------------------------\n",
    "CATALOG_PATH = '../data/catalog_clean.csv'\n",
    "df = pd.read_csv(CATALOG_PATH)\n",
    "print(f\"Loaded {len(df)} assessments from {CATALOG_PATH}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build Dense Embeddings (FAISS)\n",
    "# ----------------------------\n",
    "print(\"Creating sentence embeddings (MiniLM-L6-v2)...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "texts = df['description'].fillna('').tolist()\n",
    "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Normalize for cosine similarity\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS index and metadata\n",
    "faiss.write_index(index, '../data/embeddings.faiss')\n",
    "df.to_json('../data/meta_catalog.json', orient='records', lines=True)\n",
    "print(\"FAISS index and metadata saved.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build Sparse Index (BM25)\n",
    "# ----------------------------\n",
    "print(\"Building BM25 index...\")\n",
    "tokenized = [nltk.word_tokenize(t.lower()) for t in texts]\n",
    "bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "with open('../data/bm25.pkl', 'wb') as f:\n",
    "    pickle.dump(bm25, f)\n",
    "print(\"BM25 index saved.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Define Hybrid Search Function\n",
    "# ----------------------------\n",
    "\n",
    "def hybrid_search(query, top_k=10, w_dense=0.55, w_sparse=0.45):\n",
    "    \"\"\"\n",
    "    Performs a hybrid search combining BM25 (lexical) and FAISS (semantic) scores.\n",
    "    Returns a DataFrame with the top results.\n",
    "    \"\"\"\n",
    "    # Sparse (BM25)\n",
    "    q_tokens = nltk.word_tokenize(query.lower())\n",
    "    sparse_scores = np.array(bm25.get_scores(q_tokens))\n",
    "\n",
    "    # Dense (FAISS)\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    dense_scores, _ = index.search(q_emb, len(df))\n",
    "    dense_scores = dense_scores.flatten()\n",
    "\n",
    "    # Combine\n",
    "    final_scores = w_dense * dense_scores + w_sparse * sparse_scores\n",
    "    top_ids = np.argsort(final_scores)[::-1][:top_k]\n",
    "    return df.iloc[top_ids][['name', 'url', 'description', 'test_type']]\n",
    "\n",
    "# Example test\n",
    "print(\"\\nExample query test:\")\n",
    "example = \"Hiring for a Java developer who can collaborate with teams\"\n",
    "results = hybrid_search(example, top_k=5)\n",
    "print(results[['name', 'test_type']])\n",
    "\n",
    "# Optional: Save a preview\n",
    "results.to_csv('../data/sample_results.csv', index=False)\n",
    "print(\"Hybrid search test completed. Sample results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
